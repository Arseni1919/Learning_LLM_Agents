{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG with LlamaIndex",
   "id": "7c0b41c3e0ee1e6e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-16T11:13:55.320825Z",
     "start_time": "2025-05-16T11:13:55.313710Z"
    }
   },
   "source": [
    "import llama_index\n",
    "from accelerate.commands.config.update import description\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter  # from text to chunks\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding  # from chunks to vectors\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface_api import  HuggingFaceInferenceAPI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.evaluation import  FaithfulnessEvaluator\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:13:57.262212Z",
     "start_time": "2025-05-16T11:13:57.258857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GLOBALS\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "PHOENIX_API_KEY = os.getenv('PHOENIX_API_KEY')\n",
    "model_name = 'BAAI/bge-small-en-v1.5'\n",
    "big_model_name = 'Qwen/Qwen2.5-Coder-32B-Instruct'"
   ],
   "id": "9b8386f57fe6e407",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:13:57.692004Z",
     "start_time": "2025-05-16T11:13:57.485284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\",\n",
    "    endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")"
   ],
   "id": "8f5c56ba6257248d",
   "outputs": [],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:01.439186Z",
     "start_time": "2025-05-16T11:13:59.752035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reader = SimpleDirectoryReader(input_dir='papers')\n",
    "docs = reader.load_data()"
   ],
   "id": "8c2e151fa37711a7",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:01.450666Z",
     "start_time": "2025-05-16T11:14:01.447892Z"
    }
   },
   "cell_type": "code",
   "source": "type(docs)",
   "id": "3d24761769b7d15f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:04.887663Z",
     "start_time": "2025-05-16T11:14:01.462022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=model_name)\n",
    "    ]\n",
    ")"
   ],
   "id": "4b13ddb203c1b737",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:19.191566Z",
     "start_time": "2025-05-16T11:14:04.896078Z"
    }
   },
   "cell_type": "code",
   "source": "nodes = await pipeline.arun(documents=docs)",
   "id": "86202fe24b801d3e",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:19.220193Z",
     "start_time": "2025-05-16T11:14:19.216398Z"
    }
   },
   "cell_type": "code",
   "source": "type(nodes)",
   "id": "d9b15f330ec92291",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:21.937848Z",
     "start_time": "2025-05-16T11:14:19.233585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = chromadb.PersistentClient(path='./alfred_chroma_db')\n",
    "chroma_collection = db.get_or_create_collection('alfred')\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=model_name)\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")"
   ],
   "id": "7ce4edf4e6ffac15",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:35.571840Z",
     "start_time": "2025-05-16T11:14:21.946393Z"
    }
   },
   "cell_type": "code",
   "source": "nodes = await pipeline.arun(documents=docs)",
   "id": "9b1bc8fa6eba409e",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:51.877312Z",
     "start_time": "2025-05-16T11:14:35.591506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)"
   ],
   "id": "4180e4e41d109152",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:12:28.991546Z",
     "start_time": "2025-05-17T04:12:28.567024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = HuggingFaceInferenceAPI(model_name=big_model_name, token=HF_TOKEN)\n",
    "# llm = Ollama(model=\"gemma3:1b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"llama3.2:latest\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:8b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:1.7b\", request_timeout=120.0)"
   ],
   "id": "7c4e5e985ca207e6",
   "outputs": [],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:12:37.133118Z",
     "start_time": "2025-05-17T04:12:29.595556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    # response_mode=\"compact\",\n",
    "    # response_mode=\"refine\",\n",
    ")\n",
    "answer = query_engine.query(\"What is CGA?\")\n",
    "print(answer)"
   ],
   "id": "24a1c17aa8ec0fea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGA is an algorithm designed to solve Single-Agent Corridor Goal (SACG) problems and Multi-Agent Path Finding (MAPF) problems. It operates by selecting corridors in a graph and ensuring that agents can evacuate these corridors optimally. The algorithm prioritizes agents and plans their movements accordingly, updating plans as necessary and ensuring that all agents can reach their goals in a finite amount of time under certain conditions. CGA is implemented in a way that it can handle both single-agent and multi-agent scenarios, with specific procedures to manage corridors and agent priorities.\n"
     ]
    }
   ],
   "execution_count": 200
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:08:03.808358Z",
     "start_time": "2025-05-17T04:07:50.455305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\", llm=llm\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ],
   "id": "df600c26886f635d",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:08:27.371696Z",
     "start_time": "2025-05-17T04:08:03.823287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# query\n",
    "response = query_engine.query(\"What is CGA stands for?\")\n",
    "print(response)"
   ],
   "id": "de2a745f6cb50d05",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[189]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# query\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m response = \u001B[43mquery_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWhat is CGA stands for?\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py:52\u001B[39m, in \u001B[36mBaseQueryEngine.query\u001B[39m\u001B[34m(self, str_or_query_bundle)\u001B[39m\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     51\u001B[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m     query_result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstr_or_query_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m dispatcher.event(\n\u001B[32m     54\u001B[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001B[32m     55\u001B[39m )\n\u001B[32m     56\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m query_result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py:179\u001B[39m, in \u001B[36m_query\u001B[39m\u001B[34m(self, query_bundle)\u001B[39m\n\u001B[32m    176\u001B[39m \u001B[38;5;129m@dispatcher\u001B[39m.span\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_query\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n\u001B[32m    178\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Answer a query.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.callback_manager.event(\n\u001B[32m    180\u001B[39m         CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n\u001B[32m    181\u001B[39m     ) \u001B[38;5;28;01mas\u001B[39;00m query_event:\n\u001B[32m    182\u001B[39m         nodes = \u001B[38;5;28mself\u001B[39m.retrieve(query_bundle)\n\u001B[32m    183\u001B[39m         response = \u001B[38;5;28mself\u001B[39m._response_synthesizer.synthesize(\n\u001B[32m    184\u001B[39m             query=query_bundle,\n\u001B[32m    185\u001B[39m             nodes=nodes,\n\u001B[32m    186\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/base.py:241\u001B[39m, in \u001B[36msynthesize\u001B[39m\u001B[34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001B[39m\n\u001B[32m    235\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(query, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    236\u001B[39m     query = QueryBundle(query_str=query)\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._callback_manager.event(\n\u001B[32m    239\u001B[39m     CBEventType.SYNTHESIZE,\n\u001B[32m    240\u001B[39m     payload={EventPayload.QUERY_STR: query.query_str},\n\u001B[32m--> \u001B[39m\u001B[32m241\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m event:\n\u001B[32m    242\u001B[39m     response_str = \u001B[38;5;28mself\u001B[39m.get_response(\n\u001B[32m    243\u001B[39m         query_str=query.query_str,\n\u001B[32m    244\u001B[39m         text_chunks=[\n\u001B[32m   (...)\u001B[39m\u001B[32m    247\u001B[39m         **response_kwargs,\n\u001B[32m    248\u001B[39m     )\n\u001B[32m    250\u001B[39m     additional_source_nodes = additional_source_nodes \u001B[38;5;129;01mor\u001B[39;00m []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/tree_summarize.py:159\u001B[39m, in \u001B[36mTreeSummarize.get_response\u001B[39m\u001B[34m(self, query_str, text_chunks, **response_kwargs)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    158\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._output_cls \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m159\u001B[39m         response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_llm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m            \u001B[49m\u001B[43msummary_template\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcontext_str\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_chunks\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresponse_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    164\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    165\u001B[39m         response = \u001B[38;5;28mself\u001B[39m._llm.structured_predict(\n\u001B[32m    166\u001B[39m             \u001B[38;5;28mself\u001B[39m._output_cls,\n\u001B[32m    167\u001B[39m             summary_template,\n\u001B[32m    168\u001B[39m             context_str=text_chunks[\u001B[32m0\u001B[39m],\n\u001B[32m    169\u001B[39m             **response_kwargs,\n\u001B[32m    170\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/llms/llm.py:605\u001B[39m, in \u001B[36mpredict\u001B[39m\u001B[34m(self, prompt, **prompt_args)\u001B[39m\n\u001B[32m    581\u001B[39m \u001B[38;5;129m@dispatcher\u001B[39m.span\n\u001B[32m    582\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpredict\u001B[39m(\n\u001B[32m    583\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    584\u001B[39m     prompt: BasePromptTemplate,\n\u001B[32m    585\u001B[39m     **prompt_args: Any,\n\u001B[32m    586\u001B[39m ) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    587\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    588\u001B[39m \u001B[33;03m    Predict for a given prompt.\u001B[39;00m\n\u001B[32m    589\u001B[39m \n\u001B[32m    590\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    591\u001B[39m \u001B[33;03m        prompt (BasePromptTemplate):\u001B[39;00m\n\u001B[32m    592\u001B[39m \u001B[33;03m            The prompt to use for prediction.\u001B[39;00m\n\u001B[32m    593\u001B[39m \u001B[33;03m        prompt_args (Any):\u001B[39;00m\n\u001B[32m    594\u001B[39m \u001B[33;03m            Additional arguments to format the prompt with.\u001B[39;00m\n\u001B[32m    595\u001B[39m \n\u001B[32m    596\u001B[39m \u001B[33;03m    Returns:\u001B[39;00m\n\u001B[32m    597\u001B[39m \u001B[33;03m        str: The prediction output.\u001B[39;00m\n\u001B[32m    598\u001B[39m \n\u001B[32m    599\u001B[39m \u001B[33;03m    Examples:\u001B[39;00m\n\u001B[32m    600\u001B[39m \u001B[33;03m        ```python\u001B[39;00m\n\u001B[32m    601\u001B[39m \u001B[33;03m        from llama_index.core.prompts import PromptTemplate\u001B[39;00m\n\u001B[32m    602\u001B[39m \n\u001B[32m    603\u001B[39m \u001B[33;03m        prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\u001B[39;00m\n\u001B[32m    604\u001B[39m \u001B[33;03m        output = llm.predict(prompt, topic=\"cats\")\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m605\u001B[39m \u001B[33;03m        print(output)\u001B[39;00m\n\u001B[32m    606\u001B[39m \u001B[33;03m        ```\u001B[39;00m\n\u001B[32m    607\u001B[39m \n\u001B[32m    608\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m    609\u001B[39m     dispatcher.event(\n\u001B[32m    610\u001B[39m         LLMPredictStartEvent(template=prompt, template_args=prompt_args)\n\u001B[32m    611\u001B[39m     )\n\u001B[32m    612\u001B[39m     \u001B[38;5;28mself\u001B[39m._log_template_data(prompt, **prompt_args)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py:173\u001B[39m, in \u001B[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001B[39m\u001B[34m(_self, messages, **kwargs)\u001B[39m\n\u001B[32m    164\u001B[39m event_id = callback_manager.on_event_start(\n\u001B[32m    165\u001B[39m     CBEventType.LLM,\n\u001B[32m    166\u001B[39m     payload={\n\u001B[32m   (...)\u001B[39m\u001B[32m    170\u001B[39m     },\n\u001B[32m    171\u001B[39m )\n\u001B[32m    172\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m173\u001B[39m     f_return_val = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_self\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    175\u001B[39m     callback_manager.on_event_end(\n\u001B[32m    176\u001B[39m         CBEventType.LLM,\n\u001B[32m    177\u001B[39m         payload={EventPayload.EXCEPTION: e},\n\u001B[32m    178\u001B[39m         event_id=event_id,\n\u001B[32m    179\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/llms/ollama/base.py:322\u001B[39m, in \u001B[36mOllama.chat\u001B[39m\u001B[34m(self, messages, **kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m tools = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    320\u001B[39m \u001B[38;5;28mformat\u001B[39m = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mformat\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mjson\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.json_mode \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    323\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    324\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mollama_messages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    325\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    326\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    327\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    328\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_model_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    329\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    330\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    332\u001B[39m response = \u001B[38;5;28mdict\u001B[39m(response)\n\u001B[32m    334\u001B[39m tool_calls = response[\u001B[33m\"\u001B[39m\u001B[33mmessage\u001B[39m\u001B[33m\"\u001B[39m].get(\u001B[33m\"\u001B[39m\u001B[33mtool_calls\u001B[39m\u001B[33m\"\u001B[39m, [])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/ollama/_client.py:333\u001B[39m, in \u001B[36mClient.chat\u001B[39m\u001B[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001B[39m\n\u001B[32m    289\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mchat\u001B[39m(\n\u001B[32m    290\u001B[39m   \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    291\u001B[39m   model: \u001B[38;5;28mstr\u001B[39m = \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    298\u001B[39m   keep_alive: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    299\u001B[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001B[32m    300\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    301\u001B[39m \u001B[33;03m  Create a chat response using the requested model.\u001B[39;00m\n\u001B[32m    302\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    331\u001B[39m \u001B[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001B[39;00m\n\u001B[32m    332\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m333\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    334\u001B[39m \u001B[43m    \u001B[49m\u001B[43mChatResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    335\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mPOST\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    336\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m/api/chat\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    337\u001B[39m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[43m=\u001B[49m\u001B[43mChatRequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    338\u001B[39m \u001B[43m      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    339\u001B[39m \u001B[43m      \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_copy_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    340\u001B[39m \u001B[43m      \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtool\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtool\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_copy_tools\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtools\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    341\u001B[39m \u001B[43m      \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    342\u001B[39m \u001B[43m      \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    343\u001B[39m \u001B[43m      \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    344\u001B[39m \u001B[43m      \u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    345\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodel_dump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexclude_none\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    346\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    347\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/ollama/_client.py:178\u001B[39m, in \u001B[36mClient._request\u001B[39m\u001B[34m(self, cls, stream, *args, **kwargs)\u001B[39m\n\u001B[32m    174\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(**part)\n\u001B[32m    176\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m--> \u001B[39m\u001B[32m178\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(**\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m.json())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/ollama/_client.py:118\u001B[39m, in \u001B[36mClient._request_raw\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_request_raw\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m    117\u001B[39m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m     r = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    119\u001B[39m     r.raise_for_status()\n\u001B[32m    120\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:825\u001B[39m, in \u001B[36mClient.request\u001B[39m\u001B[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[39m\n\u001B[32m    810\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m    812\u001B[39m request = \u001B[38;5;28mself\u001B[39m.build_request(\n\u001B[32m    813\u001B[39m     method=method,\n\u001B[32m    814\u001B[39m     url=url,\n\u001B[32m   (...)\u001B[39m\u001B[32m    823\u001B[39m     extensions=extensions,\n\u001B[32m    824\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m825\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth\u001B[49m\u001B[43m=\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001B[39m, in \u001B[36mClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m    910\u001B[39m \u001B[38;5;28mself\u001B[39m._set_timeout(request)\n\u001B[32m    912\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m--> \u001B[39m\u001B[32m914\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    915\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    916\u001B[39m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[43m=\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    918\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    919\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    920\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    921\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001B[39m, in \u001B[36mClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m    939\u001B[39m request = \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    947\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    948\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001B[39m, in \u001B[36mClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m    976\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m    977\u001B[39m     hook(request)\n\u001B[32m--> \u001B[39m\u001B[32m979\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    981\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001B[39m, in \u001B[36mClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1009\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1010\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1011\u001B[39m     )\n\u001B[32m   1013\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m     response = \u001B[43mtransport\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1016\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, SyncByteStream)\n\u001B[32m   1018\u001B[39m response.request = request\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001B[39m, in \u001B[36mHTTPTransport.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    237\u001B[39m req = httpcore.Request(\n\u001B[32m    238\u001B[39m     method=request.method,\n\u001B[32m    239\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    247\u001B[39m     extensions=request.extensions,\n\u001B[32m    248\u001B[39m )\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_pool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.Iterable)\n\u001B[32m    254\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[32m    255\u001B[39m     status_code=resp.status,\n\u001B[32m    256\u001B[39m     headers=resp.headers,\n\u001B[32m    257\u001B[39m     stream=ResponseStream(resp.stream),\n\u001B[32m    258\u001B[39m     extensions=resp.extensions,\n\u001B[32m    259\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    253\u001B[39m         closing = \u001B[38;5;28mself\u001B[39m._assign_requests_to_connections()\n\u001B[32m    255\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, typing.Iterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    232\u001B[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = \u001B[43mconnection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    237\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\n\u001B[32m    238\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[32m    244\u001B[39m     pool_request.clear_connection()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001B[39m, in \u001B[36mHTTPConnection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mself\u001B[39m._connect_failed = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_connection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    134\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mresponse_closed\u001B[39m\u001B[33m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    135\u001B[39m         \u001B[38;5;28mself\u001B[39m._response_closed()\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[32m     98\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mreceive_response_headers\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs\n\u001B[32m     99\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    100\u001B[39m     (\n\u001B[32m    101\u001B[39m         http_version,\n\u001B[32m    102\u001B[39m         status,\n\u001B[32m    103\u001B[39m         reason_phrase,\n\u001B[32m    104\u001B[39m         headers,\n\u001B[32m    105\u001B[39m         trailing_data,\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m     ) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_response_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    107\u001B[39m     trace.return_value = (\n\u001B[32m    108\u001B[39m         http_version,\n\u001B[32m    109\u001B[39m         status,\n\u001B[32m    110\u001B[39m         reason_phrase,\n\u001B[32m    111\u001B[39m         headers,\n\u001B[32m    112\u001B[39m     )\n\u001B[32m    114\u001B[39m network_stream = \u001B[38;5;28mself\u001B[39m._network_stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001B[39m, in \u001B[36mHTTP11Connection._receive_response_headers\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    174\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     event = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Response):\n\u001B[32m    179\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001B[39m, in \u001B[36mHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    214\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_network_stream\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001B[39m, in \u001B[36mSyncStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    127\u001B[39m     \u001B[38;5;28mself\u001B[39m._sock.settimeout(timeout)\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:15:24.000448Z",
     "start_time": "2025-05-16T11:15:23.995547Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator = FaithfulnessEvaluator(llm=llm)",
   "id": "f5b8a446fa59d04f",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:15:34.500086Z",
     "start_time": "2025-05-16T11:15:24.010269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_result = await evaluator.aevaluate_response(response=response)\n",
    "# print(eval_result)\n",
    "print(eval_result.passing)"
   ],
   "id": "9a9cbe1a3f43f833",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "b061881f18eb7e72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tools in LlamaIndex\n",
   "id": "14e70f0590da0027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:22:30.853349Z",
     "start_time": "2025-05-17T04:22:30.601352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.tools.google import GmailToolSpec\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec"
   ],
   "id": "8e81979461bf9abd",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:00:10.060716Z",
     "start_time": "2025-05-17T04:00:10.057321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f'Getting weather for {location}')\n",
    "    return f\"The weather in {location} is sunny\"\n"
   ],
   "id": "fc3cf80ecb671e4f",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:21:46.097827Z",
     "start_time": "2025-05-17T05:21:46.086733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weather_tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name='my_weather_tool',\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    "    return_direct=False,\n",
    ")"
   ],
   "id": "b189f65fe9974f6",
   "outputs": [],
   "execution_count": 259
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:00:10.656537Z",
     "start_time": "2025-05-17T04:00:10.483044Z"
    }
   },
   "cell_type": "code",
   "source": "weather_tool.call('New York')",
   "id": "5b2c57592d638b5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='The weather in New York is sunny', tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:21:13.750621Z",
     "start_time": "2025-05-17T05:21:13.743895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine,\n",
    "    name='search in docs',\n",
    "    description='Useful to search for information in my docs.'\n",
    ")"
   ],
   "id": "14882af1f51e97c8",
   "outputs": [],
   "execution_count": 258
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:14:00.488891Z",
     "start_time": "2025-05-17T04:13:59.219099Z"
    }
   },
   "cell_type": "code",
   "source": "query_engine_tool.call(\"What is CGA stands for? Answer short.\")",
   "id": "bd6812f4d9ade318",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='CGA stands for Corridor Guided Algorithm.', tool_name='search_in_docs', raw_input={'input': 'What is CGA stands for? Answer short.'}, raw_output=Response(response='CGA stands for Corridor Guided Algorithm.', source_nodes=[NodeWithScore(node=TextNode(id_='00763113-7e2e-4de8-8575-73f2f3c79177', embedding=None, metadata={'page_label': '6', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e713b66f-133c-475e-9282-dfb4b8621161', node_type='4', metadata={'page_label': '6', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, hash='aee6dab16960a5cfc8379ced66d73bbdb6cdb00fb86aff16088d2c4cd0840b53'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0bf70773-05df-40a3-acf2-bcba7fe90108', node_type='1', metadata={}, hash='68722e667406db9beecc4ae698b44d4893ab8bfebeb2fa844252edb4b58c9f07')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='is equal to or greater than the length of the longest corridor\\nin G then CGA is guaranteed to solve any solvable instance.\\nProof outline.The Corridor Selection (CorSel) procedure in\\nCGA ensures that the main agent moves from one non-SV\\nvertex to another along an optimal path to the goal. Due to\\nLemma 1, CorEvac will successfully evacuate the corridor\\nconnecting these two non-SV vertices. Consequently, after a\\nfinite number of steps the main agent will reach its goal. □\\nNote that the requirement for completeness in Theorem 1 is\\nstrictly weaker than the requirement for PIBT SACG. That\\nis, every problem for which PIBT SACG is complete also\\nsatisfies the requirement in Theorem 1 and thus will also be\\nsolvable by CGA. This is because if a graph G has a simple\\ncycle of size 3 between every vertex, then G has no SV .\\nTheorem 2 (Reachability of CGA(L)): In CGA(L), if the\\nnumber of unoccupied vertices is larger than the longest\\ncorridor then every agent is guaranteed to reach its next goal\\nlocation in a finite amount of time.\\nProof: Following Theorem 1, the agent with highest priority\\nwill reach its goal location in a finite amount of steps, as it\\napplies CGA without any restrictions. CGA(L) assigns the\\nlowest priority to agents that has reached their goals. Thus,\\neventually every agent will be the highest priority agent and\\nreach its goal. □\\nVIII. EMPIRICAL RESULTS\\nWe conducted two sets of experimental evaluations: one\\nfor solving SACG problems and one for solving LMAPF\\nproblems. All algorithms were implemented in Python and\\nran on a MacBook Air with an Apple M1 chip and 8GB of\\nRAM.\\nA. SACG Experiments\\nThis set of experiments was performed on four different grids\\nfrom the MAPF benchmark [30]: empty-32-32, random-32-\\n32-20, maze-32-32-4, and room-32-32-4, as they present\\ndifferent levels of difficulty. The grids are visualized in\\nFig. 6. The number of agents varied from 100 to 1000.\\nSACG problems were created as follows. All agents were\\nplaced in random start locations in the given grid, and a\\ngoal location was selected randomly for the main agent.\\n25 random instances were generated in this way for every\\nnumber of agents and grid.\\nTo solve the generated SACG instances, we implemented\\nCGA and the two baselines, PrP SACG and PIBT SACG. For\\nPrP SACG we allowed 100 random restarts before declaring\\nthat no solution has been found. We considered the following\\nstandard metrics for comparison: success rate and sum-of-\\ncosts. Success rate is the number of SACG instances out of\\nall the algorithms succeeded in solving. An additional metric\\nto consider is to examine the Sum-of-costs (SoC) which is a\\nsum of all movements of agents that were needed to solve\\nSACG. SoC embodies the cost of a solution that we prefer\\nto minimize.\\nFig. 6 shows the success rate results as a function of the\\nnumber of agents. As can be seen, on all grids CGA solved\\nall instances while PIBT SACG and PrP SACG solved a\\nTABLE I: LMAPF: Throughput\\nFig. 8 Alg.', mimetype='text/plain', start_char_idx=0, end_char_idx=2960, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6663215509702934), NodeWithScore(node=TextNode(id_='9af84c2f-e5ff-45f8-b061-8b353a5e74ab', embedding=None, metadata={'page_label': '5', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b9bd14c1-f02e-4c4c-ae12-9f406b107629', node_type='4', metadata={'page_label': '5', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, hash='be79d6cf6191f8b90bf6219fd3c2f82068fbdc3dd797d7c54d232f8e3d1fe73a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ad79d3d3-3ca4-4e58-826b-feb7d4f3b652', node_type='1', metadata={}, hash='36579196fa612fb1f9067ca31ecd63f03a619d7da101f20a5df686c830660e41')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Algorithm 1 CGA\\n1: Input: ⟨a, ˜A, G:= (V, E)⟩\\n2: SV S ← create svs(G)\\n3: while True do\\n4: next steps ← CGA-Step(a, ˜A, G, SV S)\\n5: add next steps(a) to a.plan\\n6: add next steps( ˜A) to ˜A.plans\\n7: if a.last node is a.goal then\\n8: return a.plan, ˜A.plans\\n9: end if\\n10: end while\\nAlgorithm 2 CGA-Step\\n1: Input: ⟨a, ˜A, G:= (V, E), SV S⟩\\n2: Output: next steps\\n3: next steps ← empty dictionary\\n4: ev paths ← empty list\\n5: cmax ← get corridor(a, G, SV S)\\n6: if cmax is Ø then\\n7: return Ø\\n8: end if\\n9: c agents ← agents in cmax\\n10: for c a in c agents do\\n11: ev path ← get EP(c a, cmax, ˜A, G)\\n12: if ev path is Ø then\\n13: return Ø\\n14: end if\\n15: add ev path to ev paths\\n16: end for\\n17: for ev path in ev paths do\\n18: ev agents ← agents in ev path\\n19: next steps(ev agents) ←\\nmove(ev agents, ev path,\\nnext steps( ˜A))\\n20: end for\\n21: next steps(a) ←\\nmove(a, cmax, next steps( ˜A))\\n22: return next steps\\nwork, where the agents are assigned priorities and higher-\\npriority agents plan before lower-priority agents. The input\\nof CGA(L) is the graph G and a group of agents A where\\nevery agent ai ∈ A is associated with its current goal gi and\\nthe plan it is currently following πi. We refer to πi as the\\nactive plan of agent i. Initially, the active plan of all agents\\nis empty. CGA(L) is called in each time step t, outputting\\nthe next location each agent should go to and potentially\\nupdating the active plans of some agents.\\nWhen CGA(L) is called, it loops through the agents in\\norder of their priorities (lines 3-13). If the agent ai has\\nan active plan it will follow it in the next time step, and\\nCGA(L) continues to the next agent (line 4). Otherwise,\\nCGA(L) generates an active plan for ai and possibly other\\nlower-priority agents by running a single iteration of CGA\\nAlgorithm 3 CGA(L)\\n1: Input: ⟨A, G:= (V, E), SV S, t⟩\\n2: planned ← agents with an active plan\\n3: for a ∈ A do\\n4: If a ∈ planned: continue\\n5: G′ ← prohibit(planned, G)\\n6: A′ ← A \\\\ planned, a\\n7: next ← CGA-Step(a, A′, G′, SV S)\\n8: if next is Ø then\\n9: continue\\n10: end if\\n11: update(A, next)\\n12: add agents in next to planned\\n13: end for\\n14: unplanned ← A \\\\ planned\\n15: For each agent a ∈ unplanned: stay\\n16: Put finished agents at the end of A\\n(Algorithm 2) for that agent (line 7), considering previously\\nplanned paths as obstacles (line 5). If this iteration of CGA\\nsucceeds, the algorithm updates the plans of agents (line\\n11) and puts the updated agent into the planned list (line\\n12). Otherwise, i.e., when the agent cannot find a corridor\\nor cannot evacuate it (blue parts in Algorithm 2), then its\\nactive plan remains empty in this time step. Every agent\\nwith an empty active plan remains in its current location for\\nthe next time-step (lines 14-15). All agents that reach their\\ngoal location are placed at the end of the order of agents for\\nthe next iteration so that eventually every agent will enjoy\\nbeing first in the order (line 16).\\nVII. THEORETICAL RESULTS\\nFirst, we analyze the runtime of CGA. The runtime\\ncomplexity of the Corridor Selection is O(|V | + |E|) as\\nit simply runs a breadth-first search.', mimetype='text/plain', start_char_idx=0, end_char_idx=3081, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6232425734593859)], metadata={'00763113-7e2e-4de8-8575-73f2f3c79177': {'page_label': '6', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, '9af84c2f-e5ff-45f8-b061-8b353a5e74ab': {'page_label': '5', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}}), is_error=False)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:19:25.357374Z",
     "start_time": "2025-05-17T04:19:25.350637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gmail_tool_spec = GmailToolSpec()\n",
    "gmail_tool_spec_list = gmail_tool_spec.to_tool_list()\n",
    "for t in gmail_tool_spec_list:\n",
    "    print(t.metadata.name)\n",
    "    print(t.metadata.description)\n",
    "    print('---')"
   ],
   "id": "18f9385bf2939fd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data\n",
      "load_data() -> List[llama_index.core.schema.Document]\n",
      "Load emails from the user's account.\n",
      "---\n",
      "search_messages\n",
      "search_messages(query: str, max_results: Optional[int] = None)\n",
      "Searches email messages given a query string and the maximum number\n",
      "        of results requested by the user\n",
      "           Returns: List of relevant message objects up to the maximum number of results.\n",
      "\n",
      "        Args:\n",
      "            query[str]: The user's query\n",
      "            max_results (Optional[int]): The maximum number of search results\n",
      "            to return.\n",
      "        \n",
      "---\n",
      "create_draft\n",
      "create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
      "Create and insert a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "        \n",
      "---\n",
      "update_draft\n",
      "update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
      "Update a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           This function is required to be passed a draft_id that is obtained when creating messages\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "---\n",
      "get_draft\n",
      "get_draft(draft_id: str = None) -> str\n",
      "Get a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "---\n",
      "send_draft\n",
      "send_draft(draft_id: str = None) -> str\n",
      "Sends a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "---\n"
     ]
    }
   ],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:25:50.854463Z",
     "start_time": "2025-05-17T04:25:50.851370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "# We consider there is a mcp server running on 127.0.0.1:8000, or you can use the mcp client to connect to your own mcp server.\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)"
   ],
   "id": "ba98d7e049216f59",
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using Agents in LlamaIndex",
   "id": "1b57b369469fb948"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:31:59.445579Z",
     "start_time": "2025-05-17T05:31:59.431186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.core.agent.workflow import FunctionAgent, ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.workflow import Context"
   ],
   "id": "c15eadeced972a87",
   "outputs": [],
   "execution_count": 271
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:02.396547Z",
     "start_time": "2025-05-17T05:51:02.393449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# llm = HuggingFaceInferenceAPI(model_name=big_model_name, token=HF_TOKEN)\n",
    "llm = Ollama(model=\"ebdm/gemma3-enhanced:12b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"gemma3:1b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"PetrosStav/gemma3-tools:4b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"llama3.2:latest\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:8b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:1.7b\", request_timeout=120.0)"
   ],
   "id": "d4ede6393f17df6b",
   "outputs": [],
   "execution_count": 290
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:46.530379Z",
     "start_time": "2025-05-17T05:09:46.526056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers and returns the resulting integer.\"\"\"\n",
    "    return a * b"
   ],
   "id": "6ea4146873843c2c",
   "outputs": [],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:46.748745Z",
     "start_time": "2025-05-17T05:09:46.741596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [FunctionTool.from_defaults(multiply)],\n",
    "    llm=llm\n",
    ")"
   ],
   "id": "5b3cac40d9af3558",
   "outputs": [],
   "execution_count": 249
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:47.026943Z",
     "start_time": "2025-05-17T05:09:47.023720Z"
    }
   },
   "cell_type": "code",
   "source": "ctx = Context(agent)",
   "id": "ff743c92a2710192",
   "outputs": [],
   "execution_count": 250
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:53.252789Z",
     "start_time": "2025-05-17T05:09:47.400073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = await agent.run('My name is Bob.', ctx=ctx)\n",
    "response.response.content"
   ],
   "id": "61717a837c01153",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Bob! It’s nice to meet you. How can I help you today?'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:58.750425Z",
     "start_time": "2025-05-17T05:09:55.115718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = await agent.run('What is my name again?', ctx=ctx)\n",
    "response.response.content"
   ],
   "id": "12c257f311be5ed3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob. 😊'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 252
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:25:20.983618Z",
     "start_time": "2025-05-17T05:25:16.765620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)\n",
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=3) # as shown in the Components in LlamaIndex section\n",
    "\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"look in the docs\",\n",
    "    description=\"The docs file that is important for the output.\",\n",
    "    return_direct=False,\n",
    ")\n",
    "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [query_engine_tool],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \"\n",
    ")\n",
    "ctx = Context(query_engine_agent)"
   ],
   "id": "d3a05487dc75652b",
   "outputs": [],
   "execution_count": 267
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:26:28.076585Z",
     "start_time": "2025-05-17T05:26:09.898038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = await query_engine_agent.run('Use docs. What does CGA stand for?', ctx=ctx)\n",
    "print(response.response.content)"
   ],
   "id": "bdf9b2b2f863fe88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGA stands for Corridor Generation Algorithm. It’s an algorithm designed to move agents through a space, creating corridors to guide them to their destinations.\n"
     ]
    }
   ],
   "execution_count": 270
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### MAS",
   "id": "f302d98d9883b93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:11.213228Z",
     "start_time": "2025-05-17T05:51:11.209254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b"
   ],
   "id": "9929a99bf9e7a2e1",
   "outputs": [],
   "execution_count": 291
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:11.324524Z",
     "start_time": "2025-05-17T05:51:11.322282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b"
   ],
   "id": "ae0a3eb29f8b3b93",
   "outputs": [],
   "execution_count": 292
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:11.534120Z",
     "start_time": "2025-05-17T05:51:11.527792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "calculator_agent = ReActAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant. Use your tools for any math operations.\",\n",
    "    tools=[add, subtract],\n",
    "    llm=llm,\n",
    ")"
   ],
   "id": "9767e93c79b162d6",
   "outputs": [],
   "execution_count": 293
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:11.711964Z",
     "start_time": "2025-05-17T05:51:11.709077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_agent = ReActAgent(\n",
    "    name=\"docs_lookup\",\n",
    "    description=\"Looks up information inside docs.\",\n",
    "    system_prompt=\"Use your tool to query a RAG system to answer information from docs.\",\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm,\n",
    ")"
   ],
   "id": "eaa049e6daca1f85",
   "outputs": [],
   "execution_count": 294
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:58:26.570302Z",
     "start_time": "2025-05-17T05:58:26.563389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = AgentWorkflow(\n",
    "    agents=[calculator_agent, query_agent], root_agent='calculator'\n",
    ")"
   ],
   "id": "612cbf5d5b1dbfe",
   "outputs": [],
   "execution_count": 299
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:54:34.473773Z",
     "start_time": "2025-05-17T05:52:13.345559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = await agent.run(user_msg='Can you add 5 and 3?')\n",
    "# response = await agent.run(user_msg='What is DCOP? Look up in docs.')\n",
    "print(response.response.content)"
   ],
   "id": "e1b2b447eb3d0d29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot answer the question with the provided tools.\n"
     ]
    }
   ],
   "execution_count": 297
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Agentic Workflows",
   "id": "9de623c691ac0287"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T06:07:21.167820Z",
     "start_time": "2025-05-17T06:07:21.159836Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f37c2126310a4800",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T06:07:21.327896Z",
     "start_time": "2025-05-17T06:07:21.325986Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "947c91d5b1f75f9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T06:07:21.497Z",
     "start_time": "2025-05-17T06:07:21.494988Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bceaacff2b0450d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T06:07:21.655062Z",
     "start_time": "2025-05-17T06:07:21.653358Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8be094665555de85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "70a409d3f9742d86"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
