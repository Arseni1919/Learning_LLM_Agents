{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG with LlamaIndex",
   "id": "7c0b41c3e0ee1e6e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-17T17:34:19.880043Z",
     "start_time": "2025-05-17T17:34:11.624031Z"
    }
   },
   "source": [
    "import llama_index\n",
    "from accelerate.commands.config.update import description\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter  # from text to chunks\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding  # from chunks to vectors\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.huggingface_api import  HuggingFaceInferenceAPI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.evaluation import  FaithfulnessEvaluator\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T17:34:19.890419Z",
     "start_time": "2025-05-17T17:34:19.888604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# GLOBALS\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "PHOENIX_API_KEY = os.getenv('PHOENIX_API_KEY')\n",
    "model_name = 'BAAI/bge-small-en-v1.5'\n",
    "big_model_name = 'Qwen/Qwen2.5-Coder-32B-Instruct'"
   ],
   "id": "9b8386f57fe6e407",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T23:26:50.390994Z",
     "start_time": "2025-05-17T23:26:50.361511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\",\n",
    "    endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")"
   ],
   "id": "8f5c56ba6257248d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llama_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[40]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m os.environ[\u001B[33m\"\u001B[39m\u001B[33mOTEL_EXPORTER_OTLP_HEADERS\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mapi_key=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mPHOENIX_API_KEY\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mllama_index\u001B[49m.core.set_global_handler(\n\u001B[32m      3\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33marize_phoenix\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      4\u001B[39m     endpoint=\u001B[33m\"\u001B[39m\u001B[33mhttps://llamatrace.com/v1/traces\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      5\u001B[39m )\n",
      "\u001B[31mNameError\u001B[39m: name 'llama_index' is not defined"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:01.439186Z",
     "start_time": "2025-05-16T11:13:59.752035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reader = SimpleDirectoryReader(input_dir='papers')\n",
    "docs = reader.load_data()"
   ],
   "id": "8c2e151fa37711a7",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:01.450666Z",
     "start_time": "2025-05-16T11:14:01.447892Z"
    }
   },
   "cell_type": "code",
   "source": "type(docs)",
   "id": "3d24761769b7d15f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:04.887663Z",
     "start_time": "2025-05-16T11:14:01.462022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=model_name)\n",
    "    ]\n",
    ")"
   ],
   "id": "4b13ddb203c1b737",
   "outputs": [],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:19.191566Z",
     "start_time": "2025-05-16T11:14:04.896078Z"
    }
   },
   "cell_type": "code",
   "source": "nodes = await pipeline.arun(documents=docs)",
   "id": "86202fe24b801d3e",
   "outputs": [],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:19.220193Z",
     "start_time": "2025-05-16T11:14:19.216398Z"
    }
   },
   "cell_type": "code",
   "source": "type(nodes)",
   "id": "d9b15f330ec92291",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:21.937848Z",
     "start_time": "2025-05-16T11:14:19.233585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db = chromadb.PersistentClient(path='./alfred_chroma_db')\n",
    "chroma_collection = db.get_or_create_collection('alfred')\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_overlap=0),\n",
    "        HuggingFaceEmbedding(model_name=model_name)\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")"
   ],
   "id": "7ce4edf4e6ffac15",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:35.571840Z",
     "start_time": "2025-05-16T11:14:21.946393Z"
    }
   },
   "cell_type": "code",
   "source": "nodes = await pipeline.arun(documents=docs)",
   "id": "9b1bc8fa6eba409e",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:14:51.877312Z",
     "start_time": "2025-05-16T11:14:35.591506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)"
   ],
   "id": "4180e4e41d109152",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:12:28.991546Z",
     "start_time": "2025-05-17T04:12:28.567024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = HuggingFaceInferenceAPI(model_name=big_model_name, token=HF_TOKEN)\n",
    "# llm = Ollama(model=\"gemma3:1b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"llama3.2:latest\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:8b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:1.7b\", request_timeout=120.0)"
   ],
   "id": "7c4e5e985ca207e6",
   "outputs": [],
   "execution_count": 199
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:12:37.133118Z",
     "start_time": "2025-05-17T04:12:29.595556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    # response_mode=\"compact\",\n",
    "    # response_mode=\"refine\",\n",
    ")\n",
    "answer = query_engine.query(\"What is CGA?\")\n",
    "print(answer)"
   ],
   "id": "24a1c17aa8ec0fea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGA is an algorithm designed to solve Single-Agent Corridor Goal (SACG) problems and Multi-Agent Path Finding (MAPF) problems. It operates by selecting corridors in a graph and ensuring that agents can evacuate these corridors optimally. The algorithm prioritizes agents and plans their movements accordingly, updating plans as necessary and ensuring that all agents can reach their goals in a finite amount of time under certain conditions. CGA is implemented in a way that it can handle both single-agent and multi-agent scenarios, with specific procedures to manage corridors and agent priorities.\n"
     ]
    }
   ],
   "execution_count": 200
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:08:03.808358Z",
     "start_time": "2025-05-17T04:07:50.455305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)\n",
    "\n",
    "# configure retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=2,\n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\", llm=llm\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ],
   "id": "df600c26886f635d",
   "outputs": [],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:08:27.371696Z",
     "start_time": "2025-05-17T04:08:03.823287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# query\n",
    "response = query_engine.query(\"What is CGA stands for?\")\n",
    "print(response)"
   ],
   "id": "de2a745f6cb50d05",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[189]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# query\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m response = \u001B[43mquery_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mWhat is CGA stands for?\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py:52\u001B[39m, in \u001B[36mBaseQueryEngine.query\u001B[39m\u001B[34m(self, str_or_query_bundle)\u001B[39m\n\u001B[32m     50\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(str_or_query_bundle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m     51\u001B[39m         str_or_query_bundle = QueryBundle(str_or_query_bundle)\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m     query_result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_query\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstr_or_query_bundle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m dispatcher.event(\n\u001B[32m     54\u001B[39m     QueryEndEvent(query=str_or_query_bundle, response=query_result)\n\u001B[32m     55\u001B[39m )\n\u001B[32m     56\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m query_result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py:179\u001B[39m, in \u001B[36m_query\u001B[39m\u001B[34m(self, query_bundle)\u001B[39m\n\u001B[32m    176\u001B[39m \u001B[38;5;129m@dispatcher\u001B[39m.span\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_query\u001B[39m(\u001B[38;5;28mself\u001B[39m, query_bundle: QueryBundle) -> RESPONSE_TYPE:\n\u001B[32m    178\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Answer a query.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.callback_manager.event(\n\u001B[32m    180\u001B[39m         CBEventType.QUERY, payload={EventPayload.QUERY_STR: query_bundle.query_str}\n\u001B[32m    181\u001B[39m     ) \u001B[38;5;28;01mas\u001B[39;00m query_event:\n\u001B[32m    182\u001B[39m         nodes = \u001B[38;5;28mself\u001B[39m.retrieve(query_bundle)\n\u001B[32m    183\u001B[39m         response = \u001B[38;5;28mself\u001B[39m._response_synthesizer.synthesize(\n\u001B[32m    184\u001B[39m             query=query_bundle,\n\u001B[32m    185\u001B[39m             nodes=nodes,\n\u001B[32m    186\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/base.py:241\u001B[39m, in \u001B[36msynthesize\u001B[39m\u001B[34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001B[39m\n\u001B[32m    235\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(query, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    236\u001B[39m     query = QueryBundle(query_str=query)\n\u001B[32m    238\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m._callback_manager.event(\n\u001B[32m    239\u001B[39m     CBEventType.SYNTHESIZE,\n\u001B[32m    240\u001B[39m     payload={EventPayload.QUERY_STR: query.query_str},\n\u001B[32m--> \u001B[39m\u001B[32m241\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m event:\n\u001B[32m    242\u001B[39m     response_str = \u001B[38;5;28mself\u001B[39m.get_response(\n\u001B[32m    243\u001B[39m         query_str=query.query_str,\n\u001B[32m    244\u001B[39m         text_chunks=[\n\u001B[32m   (...)\u001B[39m\u001B[32m    247\u001B[39m         **response_kwargs,\n\u001B[32m    248\u001B[39m     )\n\u001B[32m    250\u001B[39m     additional_source_nodes = additional_source_nodes \u001B[38;5;129;01mor\u001B[39;00m []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/tree_summarize.py:159\u001B[39m, in \u001B[36mTreeSummarize.get_response\u001B[39m\u001B[34m(self, query_str, text_chunks, **response_kwargs)\u001B[39m\n\u001B[32m    157\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    158\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._output_cls \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m159\u001B[39m         response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_llm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[43m            \u001B[49m\u001B[43msummary_template\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    161\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcontext_str\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtext_chunks\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mresponse_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    163\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    164\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    165\u001B[39m         response = \u001B[38;5;28mself\u001B[39m._llm.structured_predict(\n\u001B[32m    166\u001B[39m             \u001B[38;5;28mself\u001B[39m._output_cls,\n\u001B[32m    167\u001B[39m             summary_template,\n\u001B[32m    168\u001B[39m             context_str=text_chunks[\u001B[32m0\u001B[39m],\n\u001B[32m    169\u001B[39m             **response_kwargs,\n\u001B[32m    170\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/llms/llm.py:605\u001B[39m, in \u001B[36mpredict\u001B[39m\u001B[34m(self, prompt, **prompt_args)\u001B[39m\n\u001B[32m    581\u001B[39m \u001B[38;5;129m@dispatcher\u001B[39m.span\n\u001B[32m    582\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpredict\u001B[39m(\n\u001B[32m    583\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    584\u001B[39m     prompt: BasePromptTemplate,\n\u001B[32m    585\u001B[39m     **prompt_args: Any,\n\u001B[32m    586\u001B[39m ) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    587\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    588\u001B[39m \u001B[33;03m    Predict for a given prompt.\u001B[39;00m\n\u001B[32m    589\u001B[39m \n\u001B[32m    590\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m    591\u001B[39m \u001B[33;03m        prompt (BasePromptTemplate):\u001B[39;00m\n\u001B[32m    592\u001B[39m \u001B[33;03m            The prompt to use for prediction.\u001B[39;00m\n\u001B[32m    593\u001B[39m \u001B[33;03m        prompt_args (Any):\u001B[39;00m\n\u001B[32m    594\u001B[39m \u001B[33;03m            Additional arguments to format the prompt with.\u001B[39;00m\n\u001B[32m    595\u001B[39m \n\u001B[32m    596\u001B[39m \u001B[33;03m    Returns:\u001B[39;00m\n\u001B[32m    597\u001B[39m \u001B[33;03m        str: The prediction output.\u001B[39;00m\n\u001B[32m    598\u001B[39m \n\u001B[32m    599\u001B[39m \u001B[33;03m    Examples:\u001B[39;00m\n\u001B[32m    600\u001B[39m \u001B[33;03m        ```python\u001B[39;00m\n\u001B[32m    601\u001B[39m \u001B[33;03m        from llama_index.core.prompts import PromptTemplate\u001B[39;00m\n\u001B[32m    602\u001B[39m \n\u001B[32m    603\u001B[39m \u001B[33;03m        prompt = PromptTemplate(\"Please write a random name related to {topic}.\")\u001B[39;00m\n\u001B[32m    604\u001B[39m \u001B[33;03m        output = llm.predict(prompt, topic=\"cats\")\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m605\u001B[39m \u001B[33;03m        print(output)\u001B[39;00m\n\u001B[32m    606\u001B[39m \u001B[33;03m        ```\u001B[39;00m\n\u001B[32m    607\u001B[39m \n\u001B[32m    608\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m    609\u001B[39m     dispatcher.event(\n\u001B[32m    610\u001B[39m         LLMPredictStartEvent(template=prompt, template_args=prompt_args)\n\u001B[32m    611\u001B[39m     )\n\u001B[32m    612\u001B[39m     \u001B[38;5;28mself\u001B[39m._log_template_data(prompt, **prompt_args)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:322\u001B[39m, in \u001B[36mwrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m      0\u001B[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py:173\u001B[39m, in \u001B[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001B[39m\u001B[34m(_self, messages, **kwargs)\u001B[39m\n\u001B[32m    164\u001B[39m event_id = callback_manager.on_event_start(\n\u001B[32m    165\u001B[39m     CBEventType.LLM,\n\u001B[32m    166\u001B[39m     payload={\n\u001B[32m   (...)\u001B[39m\u001B[32m    170\u001B[39m     },\n\u001B[32m    171\u001B[39m )\n\u001B[32m    172\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m173\u001B[39m     f_return_val = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_self\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    174\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    175\u001B[39m     callback_manager.on_event_end(\n\u001B[32m    176\u001B[39m         CBEventType.LLM,\n\u001B[32m    177\u001B[39m         payload={EventPayload.EXCEPTION: e},\n\u001B[32m    178\u001B[39m         event_id=event_id,\n\u001B[32m    179\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/llms/ollama/base.py:322\u001B[39m, in \u001B[36mOllama.chat\u001B[39m\u001B[34m(self, messages, **kwargs)\u001B[39m\n\u001B[32m    319\u001B[39m tools = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mtools\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    320\u001B[39m \u001B[38;5;28mformat\u001B[39m = kwargs.pop(\u001B[33m\"\u001B[39m\u001B[33mformat\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mjson\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.json_mode \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m322\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    323\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    324\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43mollama_messages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    325\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    326\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    327\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    328\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_model_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    329\u001B[39m \u001B[43m    \u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    330\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    332\u001B[39m response = \u001B[38;5;28mdict\u001B[39m(response)\n\u001B[32m    334\u001B[39m tool_calls = response[\u001B[33m\"\u001B[39m\u001B[33mmessage\u001B[39m\u001B[33m\"\u001B[39m].get(\u001B[33m\"\u001B[39m\u001B[33mtool_calls\u001B[39m\u001B[33m\"\u001B[39m, [])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/ollama/_client.py:333\u001B[39m, in \u001B[36mClient.chat\u001B[39m\u001B[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001B[39m\n\u001B[32m    289\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mchat\u001B[39m(\n\u001B[32m    290\u001B[39m   \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    291\u001B[39m   model: \u001B[38;5;28mstr\u001B[39m = \u001B[33m'\u001B[39m\u001B[33m'\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    298\u001B[39m   keep_alive: Optional[Union[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mstr\u001B[39m]] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    299\u001B[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001B[32m    300\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    301\u001B[39m \u001B[33;03m  Create a chat response using the requested model.\u001B[39;00m\n\u001B[32m    302\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    331\u001B[39m \u001B[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001B[39;00m\n\u001B[32m    332\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m333\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    334\u001B[39m \u001B[43m    \u001B[49m\u001B[43mChatResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    335\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mPOST\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    336\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m/api/chat\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    337\u001B[39m \u001B[43m    \u001B[49m\u001B[43mjson\u001B[49m\u001B[43m=\u001B[49m\u001B[43mChatRequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    338\u001B[39m \u001B[43m      \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    339\u001B[39m \u001B[43m      \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmessage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_copy_messages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    340\u001B[39m \u001B[43m      \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtool\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtool\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_copy_tools\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtools\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    341\u001B[39m \u001B[43m      \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    342\u001B[39m \u001B[43m      \u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mformat\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    343\u001B[39m \u001B[43m      \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    344\u001B[39m \u001B[43m      \u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m=\u001B[49m\u001B[43mkeep_alive\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    345\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmodel_dump\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexclude_none\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    346\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    347\u001B[39m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/ollama/_client.py:178\u001B[39m, in \u001B[36mClient._request\u001B[39m\u001B[34m(self, cls, stream, *args, **kwargs)\u001B[39m\n\u001B[32m    174\u001B[39m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(**part)\n\u001B[32m    176\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m inner()\n\u001B[32m--> \u001B[39m\u001B[32m178\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(**\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m.json())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/ollama/_client.py:118\u001B[39m, in \u001B[36mClient._request_raw\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    116\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_request_raw\u001B[39m(\u001B[38;5;28mself\u001B[39m, *args, **kwargs):\n\u001B[32m    117\u001B[39m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m118\u001B[39m     r = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_client\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    119\u001B[39m     r.raise_for_status()\n\u001B[32m    120\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:825\u001B[39m, in \u001B[36mClient.request\u001B[39m\u001B[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001B[39m\n\u001B[32m    810\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m    812\u001B[39m request = \u001B[38;5;28mself\u001B[39m.build_request(\n\u001B[32m    813\u001B[39m     method=method,\n\u001B[32m    814\u001B[39m     url=url,\n\u001B[32m   (...)\u001B[39m\u001B[32m    823\u001B[39m     extensions=extensions,\n\u001B[32m    824\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m825\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth\u001B[49m\u001B[43m=\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:914\u001B[39m, in \u001B[36mClient.send\u001B[39m\u001B[34m(self, request, stream, auth, follow_redirects)\u001B[39m\n\u001B[32m    910\u001B[39m \u001B[38;5;28mself\u001B[39m._set_timeout(request)\n\u001B[32m    912\u001B[39m auth = \u001B[38;5;28mself\u001B[39m._build_request_auth(request, auth)\n\u001B[32m--> \u001B[39m\u001B[32m914\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    915\u001B[39m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    916\u001B[39m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[43m=\u001B[49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    918\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    919\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    920\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    921\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:942\u001B[39m, in \u001B[36mClient._send_handling_auth\u001B[39m\u001B[34m(self, request, auth, follow_redirects, history)\u001B[39m\n\u001B[32m    939\u001B[39m request = \u001B[38;5;28mnext\u001B[39m(auth_flow)\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    943\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    944\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    945\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    946\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    947\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    948\u001B[39m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:979\u001B[39m, in \u001B[36mClient._send_handling_redirects\u001B[39m\u001B[34m(self, request, follow_redirects, history)\u001B[39m\n\u001B[32m    976\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mrequest\u001B[39m\u001B[33m\"\u001B[39m]:\n\u001B[32m    977\u001B[39m     hook(request)\n\u001B[32m--> \u001B[39m\u001B[32m979\u001B[39m response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    980\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    981\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m._event_hooks[\u001B[33m\"\u001B[39m\u001B[33mresponse\u001B[39m\u001B[33m\"\u001B[39m]:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001B[39m, in \u001B[36mClient._send_single_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m   1009\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1010\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAttempted to send an async request with a sync Client instance.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1011\u001B[39m     )\n\u001B[32m   1013\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request=request):\n\u001B[32m-> \u001B[39m\u001B[32m1014\u001B[39m     response = \u001B[43mtransport\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1016\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, SyncByteStream)\n\u001B[32m   1018\u001B[39m response.request = request\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001B[39m, in \u001B[36mHTTPTransport.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    237\u001B[39m req = httpcore.Request(\n\u001B[32m    238\u001B[39m     method=request.method,\n\u001B[32m    239\u001B[39m     url=httpcore.URL(\n\u001B[32m   (...)\u001B[39m\u001B[32m    247\u001B[39m     extensions=request.extensions,\n\u001B[32m    248\u001B[39m )\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_pool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    252\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp.stream, typing.Iterable)\n\u001B[32m    254\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m Response(\n\u001B[32m    255\u001B[39m     status_code=resp.status,\n\u001B[32m    256\u001B[39m     headers=resp.headers,\n\u001B[32m    257\u001B[39m     stream=ResponseStream(resp.stream),\n\u001B[32m    258\u001B[39m     extensions=resp.extensions,\n\u001B[32m    259\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    253\u001B[39m         closing = \u001B[38;5;28mself\u001B[39m._assign_requests_to_connections()\n\u001B[32m    255\u001B[39m     \u001B[38;5;28mself\u001B[39m._close_connections(closing)\n\u001B[32m--> \u001B[39m\u001B[32m256\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    258\u001B[39m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[32m    259\u001B[39m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n\u001B[32m    260\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response.stream, typing.Iterable)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001B[39m, in \u001B[36mConnectionPool.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    232\u001B[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001B[32m    234\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    235\u001B[39m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m236\u001B[39m     response = \u001B[43mconnection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    237\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\n\u001B[32m    238\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    239\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[32m    240\u001B[39m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[32m    241\u001B[39m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[32m    242\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    243\u001B[39m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n\u001B[32m    244\u001B[39m     pool_request.clear_connection()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001B[39m, in \u001B[36mHTTPConnection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    100\u001B[39m     \u001B[38;5;28mself\u001B[39m._connect_failed = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_connection\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    134\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[33m\"\u001B[39m\u001B[33mresponse_closed\u001B[39m\u001B[33m\"\u001B[39m, logger, request) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    135\u001B[39m         \u001B[38;5;28mself\u001B[39m._response_closed()\n\u001B[32m--> \u001B[39m\u001B[32m136\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001B[39m, in \u001B[36mHTTP11Connection.handle_request\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m     95\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m     97\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\n\u001B[32m     98\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mreceive_response_headers\u001B[39m\u001B[33m\"\u001B[39m, logger, request, kwargs\n\u001B[32m     99\u001B[39m ) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[32m    100\u001B[39m     (\n\u001B[32m    101\u001B[39m         http_version,\n\u001B[32m    102\u001B[39m         status,\n\u001B[32m    103\u001B[39m         reason_phrase,\n\u001B[32m    104\u001B[39m         headers,\n\u001B[32m    105\u001B[39m         trailing_data,\n\u001B[32m--> \u001B[39m\u001B[32m106\u001B[39m     ) = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_response_headers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    107\u001B[39m     trace.return_value = (\n\u001B[32m    108\u001B[39m         http_version,\n\u001B[32m    109\u001B[39m         status,\n\u001B[32m    110\u001B[39m         reason_phrase,\n\u001B[32m    111\u001B[39m         headers,\n\u001B[32m    112\u001B[39m     )\n\u001B[32m    114\u001B[39m network_stream = \u001B[38;5;28mself\u001B[39m._network_stream\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001B[39m, in \u001B[36mHTTP11Connection._receive_response_headers\u001B[39m\u001B[34m(self, request)\u001B[39m\n\u001B[32m    174\u001B[39m timeout = timeouts.get(\u001B[33m\"\u001B[39m\u001B[33mread\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m     event = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11.Response):\n\u001B[32m    179\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001B[39m, in \u001B[36mHTTP11Connection._receive_event\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    214\u001B[39m     event = \u001B[38;5;28mself\u001B[39m._h11_state.next_event()\n\u001B[32m    216\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11.NEED_DATA:\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_network_stream\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    218\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    219\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    221\u001B[39m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[32m    222\u001B[39m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    227\u001B[39m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[32m    228\u001B[39m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[32m    229\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m data == \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001B[39m, in \u001B[36mSyncStream.read\u001B[39m\u001B[34m(self, max_bytes, timeout)\u001B[39m\n\u001B[32m    126\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[32m    127\u001B[39m     \u001B[38;5;28mself\u001B[39m._sock.settimeout(timeout)\n\u001B[32m--> \u001B[39m\u001B[32m128\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:15:24.000448Z",
     "start_time": "2025-05-16T11:15:23.995547Z"
    }
   },
   "cell_type": "code",
   "source": "evaluator = FaithfulnessEvaluator(llm=llm)",
   "id": "f5b8a446fa59d04f",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T11:15:34.500086Z",
     "start_time": "2025-05-16T11:15:24.010269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_result = await evaluator.aevaluate_response(response=response)\n",
    "# print(eval_result)\n",
    "print(eval_result.passing)"
   ],
   "id": "9a9cbe1a3f43f833",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---",
   "id": "b061881f18eb7e72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tools in LlamaIndex\n",
   "id": "14e70f0590da0027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:22:30.853349Z",
     "start_time": "2025-05-17T04:22:30.601352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.tools.google import GmailToolSpec\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec"
   ],
   "id": "8e81979461bf9abd",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:00:10.060716Z",
     "start_time": "2025-05-17T04:00:10.057321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f'Getting weather for {location}')\n",
    "    return f\"The weather in {location} is sunny\"\n"
   ],
   "id": "fc3cf80ecb671e4f",
   "outputs": [],
   "execution_count": 177
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:21:46.097827Z",
     "start_time": "2025-05-17T05:21:46.086733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weather_tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name='my_weather_tool',\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    "    return_direct=False,\n",
    ")"
   ],
   "id": "b189f65fe9974f6",
   "outputs": [],
   "execution_count": 259
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:00:10.656537Z",
     "start_time": "2025-05-17T04:00:10.483044Z"
    }
   },
   "cell_type": "code",
   "source": "weather_tool.call('New York')",
   "id": "5b2c57592d638b5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='The weather in New York is sunny', tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:21:13.750621Z",
     "start_time": "2025-05-17T05:21:13.743895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine,\n",
    "    name='search in docs',\n",
    "    description='Useful to search for information in my docs.'\n",
    ")"
   ],
   "id": "14882af1f51e97c8",
   "outputs": [],
   "execution_count": 258
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:14:00.488891Z",
     "start_time": "2025-05-17T04:13:59.219099Z"
    }
   },
   "cell_type": "code",
   "source": "query_engine_tool.call(\"What is CGA stands for? Answer short.\")",
   "id": "bd6812f4d9ade318",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(content='CGA stands for Corridor Guided Algorithm.', tool_name='search_in_docs', raw_input={'input': 'What is CGA stands for? Answer short.'}, raw_output=Response(response='CGA stands for Corridor Guided Algorithm.', source_nodes=[NodeWithScore(node=TextNode(id_='00763113-7e2e-4de8-8575-73f2f3c79177', embedding=None, metadata={'page_label': '6', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e713b66f-133c-475e-9282-dfb4b8621161', node_type='4', metadata={'page_label': '6', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, hash='aee6dab16960a5cfc8379ced66d73bbdb6cdb00fb86aff16088d2c4cd0840b53'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0bf70773-05df-40a3-acf2-bcba7fe90108', node_type='1', metadata={}, hash='68722e667406db9beecc4ae698b44d4893ab8bfebeb2fa844252edb4b58c9f07')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='is equal to or greater than the length of the longest corridor\\nin G then CGA is guaranteed to solve any solvable instance.\\nProof outline.The Corridor Selection (CorSel) procedure in\\nCGA ensures that the main agent moves from one non-SV\\nvertex to another along an optimal path to the goal. Due to\\nLemma 1, CorEvac will successfully evacuate the corridor\\nconnecting these two non-SV vertices. Consequently, after a\\nfinite number of steps the main agent will reach its goal. \\nNote that the requirement for completeness in Theorem 1 is\\nstrictly weaker than the requirement for PIBT SACG. That\\nis, every problem for which PIBT SACG is complete also\\nsatisfies the requirement in Theorem 1 and thus will also be\\nsolvable by CGA. This is because if a graph G has a simple\\ncycle of size 3 between every vertex, then G has no SV .\\nTheorem 2 (Reachability of CGA(L)): In CGA(L), if the\\nnumber of unoccupied vertices is larger than the longest\\ncorridor then every agent is guaranteed to reach its next goal\\nlocation in a finite amount of time.\\nProof: Following Theorem 1, the agent with highest priority\\nwill reach its goal location in a finite amount of steps, as it\\napplies CGA without any restrictions. CGA(L) assigns the\\nlowest priority to agents that has reached their goals. Thus,\\neventually every agent will be the highest priority agent and\\nreach its goal. \\nVIII. EMPIRICAL RESULTS\\nWe conducted two sets of experimental evaluations: one\\nfor solving SACG problems and one for solving LMAPF\\nproblems. All algorithms were implemented in Python and\\nran on a MacBook Air with an Apple M1 chip and 8GB of\\nRAM.\\nA. SACG Experiments\\nThis set of experiments was performed on four different grids\\nfrom the MAPF benchmark [30]: empty-32-32, random-32-\\n32-20, maze-32-32-4, and room-32-32-4, as they present\\ndifferent levels of difficulty. The grids are visualized in\\nFig. 6. The number of agents varied from 100 to 1000.\\nSACG problems were created as follows. All agents were\\nplaced in random start locations in the given grid, and a\\ngoal location was selected randomly for the main agent.\\n25 random instances were generated in this way for every\\nnumber of agents and grid.\\nTo solve the generated SACG instances, we implemented\\nCGA and the two baselines, PrP SACG and PIBT SACG. For\\nPrP SACG we allowed 100 random restarts before declaring\\nthat no solution has been found. We considered the following\\nstandard metrics for comparison: success rate and sum-of-\\ncosts. Success rate is the number of SACG instances out of\\nall the algorithms succeeded in solving. An additional metric\\nto consider is to examine the Sum-of-costs (SoC) which is a\\nsum of all movements of agents that were needed to solve\\nSACG. SoC embodies the cost of a solution that we prefer\\nto minimize.\\nFig. 6 shows the success rate results as a function of the\\nnumber of agents. As can be seen, on all grids CGA solved\\nall instances while PIBT SACG and PrP SACG solved a\\nTABLE I: LMAPF: Throughput\\nFig. 8 Alg.', mimetype='text/plain', start_char_idx=0, end_char_idx=2960, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6663215509702934), NodeWithScore(node=TextNode(id_='9af84c2f-e5ff-45f8-b061-8b353a5e74ab', embedding=None, metadata={'page_label': '5', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b9bd14c1-f02e-4c4c-ae12-9f406b107629', node_type='4', metadata={'page_label': '5', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, hash='be79d6cf6191f8b90bf6219fd3c2f82068fbdc3dd797d7c54d232f8e3d1fe73a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='ad79d3d3-3ca4-4e58-826b-feb7d4f3b652', node_type='1', metadata={}, hash='36579196fa612fb1f9067ca31ecd63f03a619d7da101f20a5df686c830660e41')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Algorithm 1 CGA\\n1: Input: a, A, G:= (V, E)\\n2: SV S  create svs(G)\\n3: while True do\\n4: next steps  CGA-Step(a, A, G, SV S)\\n5: add next steps(a) to a.plan\\n6: add next steps( A) to A.plans\\n7: if a.last node is a.goal then\\n8: return a.plan, A.plans\\n9: end if\\n10: end while\\nAlgorithm 2 CGA-Step\\n1: Input: a, A, G:= (V, E), SV S\\n2: Output: next steps\\n3: next steps  empty dictionary\\n4: ev paths  empty list\\n5: cmax  get corridor(a, G, SV S)\\n6: if cmax is  then\\n7: return \\n8: end if\\n9: c agents  agents in cmax\\n10: for c a in c agents do\\n11: ev path  get EP(c a, cmax, A, G)\\n12: if ev path is  then\\n13: return \\n14: end if\\n15: add ev path to ev paths\\n16: end for\\n17: for ev path in ev paths do\\n18: ev agents  agents in ev path\\n19: next steps(ev agents) \\nmove(ev agents, ev path,\\nnext steps( A))\\n20: end for\\n21: next steps(a) \\nmove(a, cmax, next steps( A))\\n22: return next steps\\nwork, where the agents are assigned priorities and higher-\\npriority agents plan before lower-priority agents. The input\\nof CGA(L) is the graph G and a group of agents A where\\nevery agent ai  A is associated with its current goal gi and\\nthe plan it is currently following i. We refer to i as the\\nactive plan of agent i. Initially, the active plan of all agents\\nis empty. CGA(L) is called in each time step t, outputting\\nthe next location each agent should go to and potentially\\nupdating the active plans of some agents.\\nWhen CGA(L) is called, it loops through the agents in\\norder of their priorities (lines 3-13). If the agent ai has\\nan active plan it will follow it in the next time step, and\\nCGA(L) continues to the next agent (line 4). Otherwise,\\nCGA(L) generates an active plan for ai and possibly other\\nlower-priority agents by running a single iteration of CGA\\nAlgorithm 3 CGA(L)\\n1: Input: A, G:= (V, E), SV S, t\\n2: planned  agents with an active plan\\n3: for a  A do\\n4: If a  planned: continue\\n5: G  prohibit(planned, G)\\n6: A  A \\\\ planned, a\\n7: next  CGA-Step(a, A, G, SV S)\\n8: if next is  then\\n9: continue\\n10: end if\\n11: update(A, next)\\n12: add agents in next to planned\\n13: end for\\n14: unplanned  A \\\\ planned\\n15: For each agent a  unplanned: stay\\n16: Put finished agents at the end of A\\n(Algorithm 2) for that agent (line 7), considering previously\\nplanned paths as obstacles (line 5). If this iteration of CGA\\nsucceeds, the algorithm updates the plans of agents (line\\n11) and puts the updated agent into the planned list (line\\n12). Otherwise, i.e., when the agent cannot find a corridor\\nor cannot evacuate it (blue parts in Algorithm 2), then its\\nactive plan remains empty in this time step. Every agent\\nwith an empty active plan remains in its current location for\\nthe next time-step (lines 14-15). All agents that reach their\\ngoal location are placed at the end of the order of agents for\\nthe next iteration so that eventually every agent will enjoy\\nbeing first in the order (line 16).\\nVII. THEORETICAL RESULTS\\nFirst, we analyze the runtime of CGA. The runtime\\ncomplexity of the Corridor Selection is O(|V | + |E|) as\\nit simply runs a breadth-first search.', mimetype='text/plain', start_char_idx=0, end_char_idx=3081, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.6232425734593859)], metadata={'00763113-7e2e-4de8-8575-73f2f3c79177': {'page_label': '6', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}, '9af84c2f-e5ff-45f8-b061-8b353a5e74ab': {'page_label': '5', 'file_name': '2024_CGA.pdf', 'file_path': '/Users/perchik/PycharmProjects/Learning_LLM_Agents/papers/2024_CGA.pdf', 'file_type': 'application/pdf', 'file_size': 2606327, 'creation_date': '2025-05-16', 'last_modified_date': '2025-01-09'}}), is_error=False)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:19:25.357374Z",
     "start_time": "2025-05-17T04:19:25.350637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gmail_tool_spec = GmailToolSpec()\n",
    "gmail_tool_spec_list = gmail_tool_spec.to_tool_list()\n",
    "for t in gmail_tool_spec_list:\n",
    "    print(t.metadata.name)\n",
    "    print(t.metadata.description)\n",
    "    print('---')"
   ],
   "id": "18f9385bf2939fd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data\n",
      "load_data() -> List[llama_index.core.schema.Document]\n",
      "Load emails from the user's account.\n",
      "---\n",
      "search_messages\n",
      "search_messages(query: str, max_results: Optional[int] = None)\n",
      "Searches email messages given a query string and the maximum number\n",
      "        of results requested by the user\n",
      "           Returns: List of relevant message objects up to the maximum number of results.\n",
      "\n",
      "        Args:\n",
      "            query[str]: The user's query\n",
      "            max_results (Optional[int]): The maximum number of search results\n",
      "            to return.\n",
      "        \n",
      "---\n",
      "create_draft\n",
      "create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
      "Create and insert a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "        \n",
      "---\n",
      "update_draft\n",
      "update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
      "Update a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           This function is required to be passed a draft_id that is obtained when creating messages\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "---\n",
      "get_draft\n",
      "get_draft(draft_id: str = None) -> str\n",
      "Get a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "---\n",
      "send_draft\n",
      "send_draft(draft_id: str = None) -> str\n",
      "Sends a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "        \n",
      "---\n"
     ]
    }
   ],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T04:25:50.854463Z",
     "start_time": "2025-05-17T04:25:50.851370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "# We consider there is a mcp server running on 127.0.0.1:8000, or you can use the mcp client to connect to your own mcp server.\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)"
   ],
   "id": "ba98d7e049216f59",
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Using Agents in LlamaIndex",
   "id": "1b57b369469fb948"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:31:59.445579Z",
     "start_time": "2025-05-17T05:31:59.431186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.core.agent.workflow import FunctionAgent, ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.workflow import Context"
   ],
   "id": "c15eadeced972a87",
   "outputs": [],
   "execution_count": 271
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:02.396547Z",
     "start_time": "2025-05-17T05:51:02.393449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# llm = HuggingFaceInferenceAPI(model_name=big_model_name, token=HF_TOKEN)\n",
    "llm = Ollama(model=\"ebdm/gemma3-enhanced:12b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"gemma3:1b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"PetrosStav/gemma3-tools:4b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"llama3.2:latest\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:8b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:1.7b\", request_timeout=120.0)"
   ],
   "id": "d4ede6393f17df6b",
   "outputs": [],
   "execution_count": 290
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:46.530379Z",
     "start_time": "2025-05-17T05:09:46.526056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers and returns the resulting integer.\"\"\"\n",
    "    return a * b"
   ],
   "id": "6ea4146873843c2c",
   "outputs": [],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:46.748745Z",
     "start_time": "2025-05-17T05:09:46.741596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [FunctionTool.from_defaults(multiply)],\n",
    "    llm=llm\n",
    ")"
   ],
   "id": "5b3cac40d9af3558",
   "outputs": [],
   "execution_count": 249
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:47.026943Z",
     "start_time": "2025-05-17T05:09:47.023720Z"
    }
   },
   "cell_type": "code",
   "source": "ctx = Context(agent)",
   "id": "ff743c92a2710192",
   "outputs": [],
   "execution_count": 250
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:53.252789Z",
     "start_time": "2025-05-17T05:09:47.400073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = await agent.run('My name is Bob.', ctx=ctx)\n",
    "response.response.content"
   ],
   "id": "61717a837c01153",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Bob! Its nice to meet you. How can I help you today?'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:09:58.750425Z",
     "start_time": "2025-05-17T05:09:55.115718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = await agent.run('What is my name again?', ctx=ctx)\n",
    "response.response.content"
   ],
   "id": "12c257f311be5ed3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bob. '"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 252
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:25:20.983618Z",
     "start_time": "2025-05-17T05:25:16.765620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "index = VectorStoreIndex.from_documents(docs, embed_model=embed_model)\n",
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=3) # as shown in the Components in LlamaIndex section\n",
    "\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"look in the docs\",\n",
    "    description=\"The docs file that is important for the output.\",\n",
    "    return_direct=False,\n",
    ")\n",
    "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [query_engine_tool],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \"\n",
    ")\n",
    "ctx = Context(query_engine_agent)"
   ],
   "id": "d3a05487dc75652b",
   "outputs": [],
   "execution_count": 267
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:26:28.076585Z",
     "start_time": "2025-05-17T05:26:09.898038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = await query_engine_agent.run('Use docs. What does CGA stand for?', ctx=ctx)\n",
    "print(response.response.content)"
   ],
   "id": "bdf9b2b2f863fe88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CGA stands for Corridor Generation Algorithm. Its an algorithm designed to move agents through a space, creating corridors to guide them to their destinations.\n"
     ]
    }
   ],
   "execution_count": 270
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### MAS",
   "id": "f302d98d9883b93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:11.213228Z",
     "start_time": "2025-05-17T05:51:11.209254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers.\"\"\"\n",
    "    return a + b"
   ],
   "id": "9929a99bf9e7a2e1",
   "outputs": [],
   "execution_count": 291
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:11.324524Z",
     "start_time": "2025-05-17T05:51:11.322282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b"
   ],
   "id": "ae0a3eb29f8b3b93",
   "outputs": [],
   "execution_count": 292
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:11.534120Z",
     "start_time": "2025-05-17T05:51:11.527792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "calculator_agent = ReActAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant. Use your tools for any math operations.\",\n",
    "    tools=[add, subtract],\n",
    "    llm=llm,\n",
    ")"
   ],
   "id": "9767e93c79b162d6",
   "outputs": [],
   "execution_count": 293
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:51:11.711964Z",
     "start_time": "2025-05-17T05:51:11.709077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_agent = ReActAgent(\n",
    "    name=\"docs_lookup\",\n",
    "    description=\"Looks up information inside docs.\",\n",
    "    system_prompt=\"Use your tool to query a RAG system to answer information from docs.\",\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm,\n",
    ")"
   ],
   "id": "eaa049e6daca1f85",
   "outputs": [],
   "execution_count": 294
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:58:26.570302Z",
     "start_time": "2025-05-17T05:58:26.563389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = AgentWorkflow(\n",
    "    agents=[calculator_agent, query_agent], root_agent='calculator'\n",
    ")"
   ],
   "id": "612cbf5d5b1dbfe",
   "outputs": [],
   "execution_count": 299
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-17T05:54:34.473773Z",
     "start_time": "2025-05-17T05:52:13.345559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = await agent.run(user_msg='Can you add 5 and 3?')\n",
    "# response = await agent.run(user_msg='What is DCOP? Look up in docs.')\n",
    "print(response.response.content)"
   ],
   "id": "e1b2b447eb3d0d29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot answer the question with the provided tools.\n"
     ]
    }
   ],
   "execution_count": 297
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Agentic Workflows",
   "id": "9de623c691ac0287"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:18.980855Z",
     "start_time": "2025-05-18T00:50:18.081657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import llama_index\n",
    "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step, Event, Context\n",
    "import random\n",
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ],
   "id": "f37c2126310a4800",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:18.994160Z",
     "start_time": "2025-05-18T00:50:18.992193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "PHOENIX_API_KEY = os.getenv('PHOENIX_API_KEY')\n",
    "model_name = 'BAAI/bge-small-en-v1.5'\n",
    "big_model_name = 'Qwen/Qwen2.5-Coder-32B-Instruct'"
   ],
   "id": "de09a6aee880e6b5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:19.099008Z",
     "start_time": "2025-05-18T00:50:19.010933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\",\n",
    "    endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")"
   ],
   "id": "d21d278e5dcc6b6b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:19.461962Z",
     "start_time": "2025-05-18T00:50:19.459677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        # print(type(ev))\n",
    "        return StopEvent(result='Hello, World!')"
   ],
   "id": "947c91d5b1f75f9e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:20.191671Z",
     "start_time": "2025-05-18T00:50:20.189019Z"
    }
   },
   "cell_type": "code",
   "source": "w = MyWorkflow(timeout=10, verbose=False)",
   "id": "bceaacff2b0450d6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:21.066019Z",
     "start_time": "2025-05-18T00:50:20.426797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = await w.run()\n",
    "print(result)"
   ],
   "id": "8be094665555de85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:21.078696Z",
     "start_time": "2025-05-18T00:50:21.075053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str"
   ],
   "id": "f61e5f6d73e5a304",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:21.096027Z",
     "start_time": "2025-05-18T00:50:21.093011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
    "        # process initial data\n",
    "        return ProcessingEvent(intermediate_result='Step 1 complete! YEEEEP')\n",
    "    @step\n",
    "    async def ste_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # use the intermediate result\n",
    "        final_result = f'Finished at: {ev.intermediate_result}'\n",
    "        return StopEvent(result=final_result)"
   ],
   "id": "4a24c2366744fe74",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:21.164421Z",
     "start_time": "2025-05-18T00:50:21.162848Z"
    }
   },
   "cell_type": "code",
   "source": "w = MultiStepWorkflow(timeout=10, verbose=True)",
   "id": "bf6efd40e883af43",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:22.057887Z",
     "start_time": "2025-05-18T00:50:21.400344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = await w.run()\n",
    "print(result)"
   ],
   "id": "9ea5fad3a63f9f7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step step_one\n",
      "Step step_one produced event ProcessingEvent\n",
      "Running step ste_two\n",
      "Step ste_two produced event StopEvent\n",
      "Finished at: Step 1 complete! YEEEEP\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:22.067645Z",
     "start_time": "2025-05-18T00:50:22.065017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LoopEvent(Event):\n",
    "    loop_output: str"
   ],
   "id": "2f0e6aaed913c617",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:22.164536Z",
     "start_time": "2025-05-18T00:50:22.161831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print('Bad thing happened')\n",
    "            return LoopEvent(loop_output='Back to step one')\n",
    "        else:\n",
    "            print('Great thing happened')\n",
    "            return ProcessingEvent(intermediate_result='First step is done')\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        final_result = f'Finished at: {ev.intermediate_result}'\n",
    "        return StopEvent(result=final_result)"
   ],
   "id": "e32127a1cdd07143",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:22.599757Z",
     "start_time": "2025-05-18T00:50:22.597742Z"
    }
   },
   "cell_type": "code",
   "source": "w = MultiStepWorkflow(verbose=True)",
   "id": "5ac0ba77f9ba1478",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:23.929676Z",
     "start_time": "2025-05-18T00:50:23.108317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = await w.run()\n",
    "print(result)"
   ],
   "id": "d8f4e86614e64804",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step step_one\n",
      "Bad thing happened\n",
      "Step step_one produced event LoopEvent\n",
      "Running step step_one\n",
      "Great thing happened\n",
      "Step step_one produced event ProcessingEvent\n",
      "Running step step_two\n",
      "Step step_two produced event StopEvent\n",
      "Finished at: First step is done\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:27.480960Z",
     "start_time": "2025-05-18T00:50:27.352311Z"
    }
   },
   "cell_type": "code",
   "source": "draw_all_possible_flows(w, 'flow.html')",
   "id": "9cf71f767fb54fc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow.html\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:27.549337Z",
     "start_time": "2025-05-18T00:50:27.547536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b"
   ],
   "id": "785b238d4129a040",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:28.081248Z",
     "start_time": "2025-05-18T00:50:27.760967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = HuggingFaceInferenceAPI(model_name=big_model_name)\n",
    "# llm = Ollama(model=\"gemma3:1b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"llama3.2:latest\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:8b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen3:1.7b\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"mistral\", request_timeout=120.0)  # 4.1 GB\n",
    "# llm = Ollama(model=\"mistral:7b-instruct-q4_K_M\", request_timeout=120.0)\n",
    "# llm = Ollama(model=\"qwen2.5:0.5b\", request_timeout=120.0)  # 398 MB"
   ],
   "id": "ed2cfa077b8fd284",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:29.557658Z",
     "start_time": "2025-05-18T00:50:29.552635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "multiply_agent = ReActAgent(\n",
    "    name='multiply_agent',\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    tools=[multiply],\n",
    "    llm=llm,\n",
    ")\n",
    "addition_agent = ReActAgent(\n",
    "    name='add_agent',\n",
    "    description=\"Is able to add two integers\",\n",
    "    tools=[add],\n",
    "    llm=llm,\n",
    ")"
   ],
   "id": "d002d3f192c44cb4",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:50:29.731657Z",
     "start_time": "2025-05-18T00:50:29.728873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import ssl\n",
    "# import certifi\n",
    "#\n",
    "# ssl_context = ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent='multiply_agent',\n",
    ")"
   ],
   "id": "2726bfd0ba113164",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:52:12.293330Z",
     "start_time": "2025-05-18T00:52:11.195405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "response = await workflow.run(user_msg='Can you add 5 and 3?')\n",
    "print(response)"
   ],
   "id": "a8f6f599a901de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[31mERROR: Exception in callback Dispatcher.span.<locals>.wrapper.<locals>.handle_future_result(span_id='Workflow.run...-1d1739da2317', bound_args=<BoundArgumen...StartEvent())>, instance=<llama_index....t 0x10cbac050>, context=<_contextvars...t 0x10f3f28c0>)(<WorkflowHand....c:1000)')]\")>) at /Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:276\n",
       "handle: <Handle Dispatcher.span.<locals>.wrapper.<locals>.handle_future_result(span_id='Workflow.run...-1d1739da2317', bound_args=<BoundArgumen...StartEvent())>, instance=<llama_index....t 0x10cbac050>, context=<_contextvars...t 0x10f3f28c0>)(<WorkflowHand....c:1000)')]\")>) at /Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:276>\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1122, in _wrap_create_connection\n",
       "    return await self._loop.create_connection(*args, **kwargs, sock=sock)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1126, in create_connection\n",
       "    transport, protocol = await self._create_connection_transport(\n",
       "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1159, in _create_connection_transport\n",
       "    await waiter\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/sslproto.py\", line 575, in _on_handshake_complete\n",
       "    raise handshake_exc\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/sslproto.py\", line 557, in _do_handshake\n",
       "    self._sslobj.do_handshake()\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py\", line 917, in do_handshake\n",
       "    self._sslobj.do_handshake()\n",
       "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py\", line 618, in _step_worker\n",
       "    new_ev = await instrumented_step(**kwargs)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 370, in async_wrapper\n",
       "    result = await func(*args, **kwargs)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/multi_agent_workflow.py\", line 394, in run_agent_step\n",
       "    agent_output = await agent.take_step(\n",
       "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/react_agent.py\", line 101, in take_step\n",
       "    async for last_chat_response in response:\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/llms/huggingface_api/base.py\", line 440, in gen\n",
       "    async for chunk in await self._async_client.chat_completion(\n",
       "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py\", line 1032, in chat_completion\n",
       "    data = await self._inner_post(request_parameters, stream=stream)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py\", line 344, in _inner_post\n",
       "    response = await session.post(\n",
       "               ^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3424, in _request\n",
       "    response = await session._wrapped_request(method, url, **kwargs)\n",
       "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n",
       "    conn = await self._connector.connect(\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n",
       "    proto = await self._create_connection(req, traces, timeout)\n",
       "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n",
       "    _, proto = await self._create_direct_connection(req, traces, timeout)\n",
       "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1400, in _create_direct_connection\n",
       "    raise last_exc\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1369, in _create_direct_connection\n",
       "    transp, proto = await self._wrap_create_connection(\n",
       "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1124, in _wrap_create_connection\n",
       "    raise ClientConnectorCertificateError(req.connection_key, exc) from exc\n",
       "aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host router.huggingface.co:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')]\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
       "    self._context.run(self._callback, *self._args)\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 288, in handle_future_result\n",
       "    raise exception\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/workflow/workflow.py\", line 403, in _run_workflow\n",
       "    raise exception_raised\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py\", line 627, in _step_worker\n",
       "    raise WorkflowRuntimeError(\n",
       "llama_index.core.workflow.errors.WorkflowRuntimeError: Error in step 'run_agent_step': Cannot connect to host router.huggingface.co:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')]\u001B[0m\u001B[31m\n",
       "\u001B[0m"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">ERROR: Exception in callback Dispatcher.span.&lt;locals&gt;.wrapper.&lt;locals&gt;.handle_future_result(span_id='Workflow.run...-1d1739da2317', bound_args=&lt;BoundArgumen...StartEvent())&gt;, instance=&lt;llama_index....t 0x10cbac050&gt;, context=&lt;_contextvars...t 0x10f3f28c0&gt;)(&lt;WorkflowHand....c:1000)')]\")&gt;) at /Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:276\n",
       "handle: &lt;Handle Dispatcher.span.&lt;locals&gt;.wrapper.&lt;locals&gt;.handle_future_result(span_id='Workflow.run...-1d1739da2317', bound_args=&lt;BoundArgumen...StartEvent())&gt;, instance=&lt;llama_index....t 0x10cbac050&gt;, context=&lt;_contextvars...t 0x10f3f28c0&gt;)(&lt;WorkflowHand....c:1000)')]\")&gt;) at /Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:276&gt;\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1122, in _wrap_create_connection\n",
       "    return await self._loop.create_connection(*args, **kwargs, sock=sock)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1126, in create_connection\n",
       "    transport, protocol = await self._create_connection_transport(\n",
       "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1159, in _create_connection_transport\n",
       "    await waiter\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/sslproto.py\", line 575, in _on_handshake_complete\n",
       "    raise handshake_exc\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/sslproto.py\", line 557, in _do_handshake\n",
       "    self._sslobj.do_handshake()\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py\", line 917, in do_handshake\n",
       "    self._sslobj.do_handshake()\n",
       "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py\", line 618, in _step_worker\n",
       "    new_ev = await instrumented_step(**kwargs)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 370, in async_wrapper\n",
       "    result = await func(*args, **kwargs)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/multi_agent_workflow.py\", line 394, in run_agent_step\n",
       "    agent_output = await agent.take_step(\n",
       "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/react_agent.py\", line 101, in take_step\n",
       "    async for last_chat_response in response:\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/llms/huggingface_api/base.py\", line 440, in gen\n",
       "    async for chunk in await self._async_client.chat_completion(\n",
       "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py\", line 1032, in chat_completion\n",
       "    data = await self._inner_post(request_parameters, stream=stream)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py\", line 344, in _inner_post\n",
       "    response = await session.post(\n",
       "               ^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py\", line 3424, in _request\n",
       "    response = await session._wrapped_request(method, url, **kwargs)\n",
       "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n",
       "    conn = await self._connector.connect(\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n",
       "    proto = await self._create_connection(req, traces, timeout)\n",
       "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n",
       "    _, proto = await self._create_direct_connection(req, traces, timeout)\n",
       "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1400, in _create_direct_connection\n",
       "    raise last_exc\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1369, in _create_direct_connection\n",
       "    transp, proto = await self._wrap_create_connection(\n",
       "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1124, in _wrap_create_connection\n",
       "    raise ClientConnectorCertificateError(req.connection_key, exc) from exc\n",
       "aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host router.huggingface.co:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')]\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
       "    self._context.run(self._callback, *self._args)\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py\", line 288, in handle_future_result\n",
       "    raise exception\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/workflow/workflow.py\", line 403, in _run_workflow\n",
       "    raise exception_raised\n",
       "  File \"/Users/perchik/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py\", line 627, in _step_worker\n",
       "    raise WorkflowRuntimeError(\n",
       "llama_index.core.workflow.errors.WorkflowRuntimeError: Error in step 'run_agent_step': Cannot connect to host router.huggingface.co:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')]\n",
       "</span></pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'run_agent_step': Cannot connect to host router.huggingface.co:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')]",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mSSLCertVerificationError\u001B[39m                  Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1122\u001B[39m, in \u001B[36mTCPConnector._wrap_create_connection\u001B[39m\u001B[34m(self, addr_infos, req, timeout, client_error, *args, **kwargs)\u001B[39m\n\u001B[32m   1115\u001B[39m         sock = \u001B[38;5;28;01mawait\u001B[39;00m aiohappyeyeballs.start_connection(\n\u001B[32m   1116\u001B[39m             addr_infos=addr_infos,\n\u001B[32m   1117\u001B[39m             local_addr_infos=\u001B[38;5;28mself\u001B[39m._local_addr_infos,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1120\u001B[39m             loop=\u001B[38;5;28mself\u001B[39m._loop,\n\u001B[32m   1121\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1122\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._loop.create_connection(*args, **kwargs, sock=sock)\n\u001B[32m   1123\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m cert_errors \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py:1126\u001B[39m, in \u001B[36mBaseEventLoop.create_connection\u001B[39m\u001B[34m(self, protocol_factory, host, port, ssl, family, proto, flags, sock, local_addr, server_hostname, ssl_handshake_timeout, ssl_shutdown_timeout, happy_eyeballs_delay, interleave, all_errors)\u001B[39m\n\u001B[32m   1123\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1124\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mA Stream Socket was expected, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msock\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1126\u001B[39m transport, protocol = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_connection_transport(\n\u001B[32m   1127\u001B[39m     sock, protocol_factory, ssl, server_hostname,\n\u001B[32m   1128\u001B[39m     ssl_handshake_timeout=ssl_handshake_timeout,\n\u001B[32m   1129\u001B[39m     ssl_shutdown_timeout=ssl_shutdown_timeout)\n\u001B[32m   1130\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._debug:\n\u001B[32m   1131\u001B[39m     \u001B[38;5;66;03m# Get the socket from the transport because SSL transport closes\u001B[39;00m\n\u001B[32m   1132\u001B[39m     \u001B[38;5;66;03m# the old socket and creates a new SSL socket\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py:1159\u001B[39m, in \u001B[36mBaseEventLoop._create_connection_transport\u001B[39m\u001B[34m(self, sock, protocol_factory, ssl, server_hostname, server_side, ssl_handshake_timeout, ssl_shutdown_timeout)\u001B[39m\n\u001B[32m   1158\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1159\u001B[39m     \u001B[38;5;28;01mawait\u001B[39;00m waiter\n\u001B[32m   1160\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/sslproto.py:575\u001B[39m, in \u001B[36mSSLProtocol._on_handshake_complete\u001B[39m\u001B[34m(self, handshake_exc)\u001B[39m\n\u001B[32m    574\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m575\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m handshake_exc\n\u001B[32m    577\u001B[39m peercert = sslobj.getpeercert()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/sslproto.py:557\u001B[39m, in \u001B[36mSSLProtocol._do_handshake\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    556\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m557\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m SSLAgainErrors:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:917\u001B[39m, in \u001B[36mSSLObject.do_handshake\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    916\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Start the SSL/TLS handshake.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m917\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdo_handshake\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mSSLCertVerificationError\u001B[39m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mClientConnectorCertificateError\u001B[39m           Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py:618\u001B[39m, in \u001B[36mContext._step_worker\u001B[39m\u001B[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, dispatcher)\u001B[39m\n\u001B[32m    617\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m618\u001B[39m     new_ev = \u001B[38;5;28;01mawait\u001B[39;00m instrumented_step(**kwargs)\n\u001B[32m    619\u001B[39m     kwargs.clear()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:370\u001B[39m, in \u001B[36mDispatcher.span.<locals>.async_wrapper\u001B[39m\u001B[34m(func, instance, args, kwargs)\u001B[39m\n\u001B[32m    369\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m370\u001B[39m     result = \u001B[38;5;28;01mawait\u001B[39;00m func(*args, **kwargs)\n\u001B[32m    371\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/multi_agent_workflow.py:394\u001B[39m, in \u001B[36mAgentWorkflow.run_agent_step\u001B[39m\u001B[34m(self, ctx, ev)\u001B[39m\n\u001B[32m    392\u001B[39m tools = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m.get_tools(ev.current_agent_name, user_msg_str \u001B[38;5;129;01mor\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m394\u001B[39m agent_output = \u001B[38;5;28;01mawait\u001B[39;00m agent.take_step(\n\u001B[32m    395\u001B[39m     ctx,\n\u001B[32m    396\u001B[39m     ev.input,\n\u001B[32m    397\u001B[39m     tools,\n\u001B[32m    398\u001B[39m     memory,\n\u001B[32m    399\u001B[39m )\n\u001B[32m    401\u001B[39m ctx.write_event_to_stream(agent_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/react_agent.py:101\u001B[39m, in \u001B[36mReActAgent.take_step\u001B[39m\u001B[34m(self, ctx, llm_input, tools, memory)\u001B[39m\n\u001B[32m    100\u001B[39m last_chat_response = ChatResponse(message=ChatMessage())\n\u001B[32m--> \u001B[39m\u001B[32m101\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m last_chat_response \u001B[38;5;129;01min\u001B[39;00m response:\n\u001B[32m    102\u001B[39m     raw = (\n\u001B[32m    103\u001B[39m         last_chat_response.raw.model_dump()\n\u001B[32m    104\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(last_chat_response.raw, BaseModel)\n\u001B[32m    105\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m last_chat_response.raw\n\u001B[32m    106\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/llms/huggingface_api/base.py:440\u001B[39m, in \u001B[36mHuggingFaceInferenceAPI.astream_chat.<locals>.gen\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    439\u001B[39m cur_index = -\u001B[32m1\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m440\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._async_client.chat_completion(\n\u001B[32m    441\u001B[39m     messages=\u001B[38;5;28mself\u001B[39m._to_huggingface_messages(messages),\n\u001B[32m    442\u001B[39m     stream=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    443\u001B[39m     **model_kwargs,\n\u001B[32m    444\u001B[39m ):\n\u001B[32m    445\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m chunk.choices[\u001B[32m0\u001B[39m].finish_reason \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py:1032\u001B[39m, in \u001B[36mAsyncInferenceClient.chat_completion\u001B[39m\u001B[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001B[39m\n\u001B[32m   1025\u001B[39m request_parameters = provider_helper.prepare_request(\n\u001B[32m   1026\u001B[39m     inputs=messages,\n\u001B[32m   1027\u001B[39m     parameters=parameters,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1030\u001B[39m     api_key=\u001B[38;5;28mself\u001B[39m.token,\n\u001B[32m   1031\u001B[39m )\n\u001B[32m-> \u001B[39m\u001B[32m1032\u001B[39m data = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._inner_post(request_parameters, stream=stream)\n\u001B[32m   1034\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py:344\u001B[39m, in \u001B[36mAsyncInferenceClient._inner_post\u001B[39m\u001B[34m(self, request_parameters, stream)\u001B[39m\n\u001B[32m    343\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m344\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m session.post(\n\u001B[32m    345\u001B[39m         request_parameters.url, json=request_parameters.json, data=data_as_binary, proxy=\u001B[38;5;28mself\u001B[39m.proxies\n\u001B[32m    346\u001B[39m     )\n\u001B[32m    347\u001B[39m     response_error_payload = \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/huggingface_hub/inference/_generated/_async_client.py:3424\u001B[39m, in \u001B[36mAsyncInferenceClient._get_client_session.<locals>._request\u001B[39m\u001B[34m(method, url, **kwargs)\u001B[39m\n\u001B[32m   3423\u001B[39m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_request\u001B[39m(method, url, **kwargs):\n\u001B[32m-> \u001B[39m\u001B[32m3424\u001B[39m     response = \u001B[38;5;28;01mawait\u001B[39;00m session._wrapped_request(method, url, **kwargs)\n\u001B[32m   3425\u001B[39m     \u001B[38;5;28mself\u001B[39m._sessions[session].add(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/client.py:703\u001B[39m, in \u001B[36mClientSession._request\u001B[39m\u001B[34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001B[39m\n\u001B[32m    702\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m703\u001B[39m     conn = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._connector.connect(\n\u001B[32m    704\u001B[39m         req, traces=traces, timeout=real_timeout\n\u001B[32m    705\u001B[39m     )\n\u001B[32m    706\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m asyncio.TimeoutError \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py:548\u001B[39m, in \u001B[36mBaseConnector.connect\u001B[39m\u001B[34m(self, req, traces, timeout)\u001B[39m\n\u001B[32m    547\u001B[39m         \u001B[38;5;28;01mawait\u001B[39;00m trace.send_connection_create_start()\n\u001B[32m--> \u001B[39m\u001B[32m548\u001B[39m proto = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_connection(req, traces, timeout)\n\u001B[32m    549\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m traces:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1056\u001B[39m, in \u001B[36mTCPConnector._create_connection\u001B[39m\u001B[34m(self, req, traces, timeout)\u001B[39m\n\u001B[32m   1055\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1056\u001B[39m     _, proto = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_direct_connection(req, traces, timeout)\n\u001B[32m   1058\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m proto\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1400\u001B[39m, in \u001B[36mTCPConnector._create_direct_connection\u001B[39m\u001B[34m(self, req, traces, timeout, client_error)\u001B[39m\n\u001B[32m   1399\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m last_exc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1400\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m last_exc\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1369\u001B[39m, in \u001B[36mTCPConnector._create_direct_connection\u001B[39m\u001B[34m(self, req, traces, timeout, client_error)\u001B[39m\n\u001B[32m   1368\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1369\u001B[39m     transp, proto = \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m._wrap_create_connection(\n\u001B[32m   1370\u001B[39m         \u001B[38;5;28mself\u001B[39m._factory,\n\u001B[32m   1371\u001B[39m         timeout=timeout,\n\u001B[32m   1372\u001B[39m         ssl=sslcontext,\n\u001B[32m   1373\u001B[39m         addr_infos=addr_infos,\n\u001B[32m   1374\u001B[39m         server_hostname=server_hostname,\n\u001B[32m   1375\u001B[39m         req=req,\n\u001B[32m   1376\u001B[39m         client_error=client_error,\n\u001B[32m   1377\u001B[39m     )\n\u001B[32m   1378\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (ClientConnectorError, asyncio.TimeoutError) \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1124\u001B[39m, in \u001B[36mTCPConnector._wrap_create_connection\u001B[39m\u001B[34m(self, addr_infos, req, timeout, client_error, *args, **kwargs)\u001B[39m\n\u001B[32m   1123\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m cert_errors \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[32m-> \u001B[39m\u001B[32m1124\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m ClientConnectorCertificateError(req.connection_key, exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mexc\u001B[39;00m\n\u001B[32m   1125\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ssl_errors \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "\u001B[31mClientConnectorCertificateError\u001B[39m: Cannot connect to host router.huggingface.co:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mWorkflowRuntimeError\u001B[39m                      Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m response = \u001B[38;5;28;01mawait\u001B[39;00m workflow.run(user_msg=\u001B[33m'\u001B[39m\u001B[33mCan you add 5 and 3?\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      2\u001B[39m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/workflow/workflow.py:403\u001B[39m, in \u001B[36mWorkflow.run.<locals>._run_workflow\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    399\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m exception_raised:\n\u001B[32m    400\u001B[39m     \u001B[38;5;66;03m# cancel the stream\u001B[39;00m\n\u001B[32m    401\u001B[39m     ctx.write_event_to_stream(StopEvent())\n\u001B[32m--> \u001B[39m\u001B[32m403\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exception_raised\n\u001B[32m    405\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m we_done:\n\u001B[32m    406\u001B[39m     \u001B[38;5;66;03m# cancel the stream\u001B[39;00m\n\u001B[32m    407\u001B[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Learning_LLMs/.venv/lib/python3.12/site-packages/llama_index/core/workflow/context.py:627\u001B[39m, in \u001B[36mContext._step_worker\u001B[39m\u001B[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, dispatcher)\u001B[39m\n\u001B[32m    625\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    626\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m config.retry_policy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m627\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m WorkflowRuntimeError(\n\u001B[32m    628\u001B[39m             \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mError in step \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m!s}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    629\u001B[39m         ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m    631\u001B[39m     delay = config.retry_policy.next(\n\u001B[32m    632\u001B[39m         retry_start_at + time.time(), attempts, e\n\u001B[32m    633\u001B[39m     )\n\u001B[32m    634\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m delay \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    635\u001B[39m         \u001B[38;5;66;03m# We're done retrying\u001B[39;00m\n",
      "\u001B[31mWorkflowRuntimeError\u001B[39m: Error in step 'run_agent_step': Cannot connect to host router.huggingface.co:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')]"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:44:44.219896Z",
     "start_time": "2025-05-18T00:44:43.985224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import certifi\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"https://huggingface.co\", verify=certifi.where())\n"
   ],
   "id": "48c0f058f64b5980",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:46:25.355560Z",
     "start_time": "2025-05-18T00:46:25.353557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ssl\n",
    "import certifi\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())"
   ],
   "id": "9ad3c86fd48d8c33",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T00:51:56.709967Z",
     "start_time": "2025-05-18T00:51:56.694847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import ssl\n",
    "import certifi\n",
    "\n",
    "def get_secure_context():\n",
    "    try:\n",
    "        context = ssl.create_default_context(cafile=certifi.where())\n",
    "        return context\n",
    "    except Exception as e:\n",
    "        # Fallback to default SSL context if certifi is not available\n",
    "        return ssl.create_default_context()\n",
    "\n",
    "# Usage\n",
    "ssl_context = get_secure_context()"
   ],
   "id": "5e7dd72347395ad7",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d30ba3f287c1aa0e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
